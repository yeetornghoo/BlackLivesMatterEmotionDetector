# -*- coding: utf-8 -*-
"""BERT-PART4-CLASSIFICATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sj4xmV7xKKeumaHb5kDiUCWTJnlH4YSP

# Benchmark Dataset: Baltimore
This is the google colab to used the trained baseline model to verify on the benchmark dataset to make sure the accuracy

## 1.0 IMPORT LIBRARIES
"""

#!pip install transformers
import transformers
import torch
import copy
import numpy as np
import requests
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from torch import nn, optim
from torch.utils import data
from collections import defaultdict
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# Commented out IPython magic to ensure Python compatibility.
# SETTING
# %matplotlib inline
# %config InlineBackend.figure_format='retina'
sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 8,6

moods_code = [["fear", 0], ["anger", 1], ["sadness", 2], ["joy", 3], ["anticipation", 4], ["trust", 5], ["disgust", 6], ["surprise", 7]]
moods = ['fear', 'anger', 'sadness', 'joy', 'anticipation', 'trust', 'disgust', 'surprise']

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

"""## 2.0 DOWNLOAD AND LOAD DATASET
This step will load the individual benchmark dataset to ve verified by trained model
"""

def download_dataset(benchmark_name):
  url="https://raw.githubusercontent.com/yeetornghoo/SocialMovementSentiment/master/dataset/unlabeled/{}/baseline-dataset.csv".format(benchmark_name)
  r = requests.get(url)
  with open("baseline-dataset.csv", 'wb') as f:
    f.write(r.content)
    
  return pd.read_csv("baseline-dataset.csv", sep=",")

def compare_str(ori_str, to_str, to_code):
    if ori_str == to_str:
        return str(to_code)
    return ori_str

def convert_mood_class(idf):
    # CONVERT MOOD TO CODE
    for mood_itm in moods_code:
        idf["sentiment"] = idf["sentiment"].apply(lambda x: compare_str(str(x), mood_itm[0], mood_itm[1]))

    idf = idf.astype({"sentiment": int})
    return idf

#BASELINE DATASET
benchmark_name = "blm_baltimore"
df = download_dataset(benchmark_name)
df = df[["sentiment", "tweet_text"]]
df = df[(df["sentiment"]!="surprise") & (df["sentiment"]!="disgust")]
print(len(df))

# PLOT BAR CHART
sns.countplot(df.sentiment)
plt.xlabel("Number of tweets by score (With Class Names)")
plt.savefig("{}_classname_tweet_count.png".format(benchmark_name).lower())

# CONVERT CLASS TO INTEGER
df = convert_mood_class(df)

# PLOT CONVERTED BAR CHART
sns.countplot(df.sentiment)
plt.xlabel("Number of tweets by score (With Class Code)")
plt.savefig("{}_classcode_tweet_count.png".format(benchmark_name).lower())

"""## 3.0 DATA PREPROCESSING
Data preprocessing by using Bert token standard
### a. Initiaze Bert Pretrained Token
"""

# LOAD TOKENIZER
PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
tokenizer = transformers.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

"""### b. Understand the token lenght distribution"""

token_lens = []

for txt in df.tweet_text:
  tokens = tokenizer.encode(txt, max_length=512)
  token_lens.append(len(tokens))

sns.distplot(token_lens)
plt.savefig("{}_tweet_token_count.png".format(benchmark_name).lower())

"""## 4.0 CONVERT DATASET TO BERT MODEL"""

class TweetTextDataset(Dataset):
    def __init__(self, tweet_texts, targets, tokenizer, max_len):
        self.tweet_texts = tweet_texts
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.tweet_texts)

    def __getitem__(self, item):
        tweet_text = str(self.tweet_texts[item])
        target = self.targets[item]

        encoding = self.tokenizer.encode_plus(
            tweet_text,
            add_special_tokens=True,
            max_length=self.max_len,
            truncation=True,
            padding='max_length',
            return_token_type_ids=False,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'tweet_text': tweet_text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'targets': torch.tensor(target, dtype=torch.long)
        }

def create_data_loader(df, tokenizer, max_len, batch_size):
    ds = TweetTextDataset(
        tweet_texts=df.tweet_text.to_numpy(),
        targets=df.sentiment.to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len
    )
    return data.DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=4
    )

MAX_LEN = 100
BATCH_SIZE = 16
EPOCHS = 5
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

test_data_loader = create_data_loader(df, tokenizer, MAX_LEN, BATCH_SIZE)

"""## 5.0 BERT Trained Model
Download the trained model by using the id from [Google Drive](https://drive.google.com/drive/u/1/folders/1eFSKBt9CWaeklPcfIJ_HuKTmrJSONo-I), and load the model
"""

class SentimentClassifier(nn.Module):
    def __init__(self, n_classes):
        super(SentimentClassifier, self).__init__()
        self.bert = transformers.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, input_ids, attention_mask):
        _, pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        output = self.drop(pooled_output)
        output = self.out(output)
        return self.softmax(output)

#!gdown --id 1a93M4ua2eyAq16rrQlDDTWolvKvDd7Xm

model = SentimentClassifier(len(moods))
model.load_state_dict(torch.load("best_model_state.bin", map_location=torch.device('cpu')))
model = model.to(device)

"""## 6.0 Bench dataset classification"""

def get_predictions(model, data_loader):
    model = model.eval()
  
    tweet_texts = []
    predictions = []
    prediction_probs = []
    real_values = []

    with torch.no_grad():
        for d in data_loader:
            texts = d["tweet_text"]
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            targets = d["targets"].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            
            _, preds = torch.max(outputs, dim=1)
            probs = F.softmax(outputs, dim=1)

            tweet_texts.extend(texts)
            predictions.extend(preds)
            prediction_probs.extend(probs)
            real_values.extend(targets)

    predictions = torch.stack(predictions).cpu()
    prediction_probs = torch.stack(prediction_probs).cpu()
    real_values = torch.stack(real_values).cpu()
    return tweet_texts, predictions, prediction_probs, real_values

y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, target_names=moods))

from sklearn.metrics import confusion_matrix

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');
  plt.savefig("{}_confusion_matrix.png".format(benchmark_name).lower())

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=moods, columns=moods)
show_confusion_matrix(df_cm)

row_id = 9
print("Index: {}".format(row_id))
print("Tweet:" + y_tweet_texts[row_id])
print("True Value: {}".format(y_test.data[row_id].item()))
print("Predicted Value: {}".format(y_pred.data[row_id].item()))


df["predicted"] = y_pred.data
print("\n------------------\n")
print(df.head(10))

#df_tmp = df[df["sentiment"] != df["predicted"]]
df_group = df.groupby(['sentiment','predicted']).count()
print(df_group)